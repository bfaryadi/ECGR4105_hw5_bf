{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.svm import SVR"
      ],
      "metadata": {
        "id": "U8PvJXcnV4hi"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "1ZkYl72mTUJS"
      },
      "outputs": [],
      "source": [
        "t_c = [0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0, 6.0, 13.0, 21.0]\n",
        "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\n",
        "t_c = torch.tensor(t_c)\n",
        "t_u = torch.tensor(t_u)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_model(t_u, w, b):\n",
        "  return w*t_u +  b\n",
        "\n",
        "def quadratic_model(t_u, w2, w1, b):\n",
        "  return (w2*t_u)**2 + (w1*t_u)**2 +  b\n",
        "\n",
        "# t_p = predictions, t_c = ground truths\n",
        "def loss_fn(t_p, t_c):\n",
        "  squared_diffs = (t_p - t_c)**2\n",
        "  return squared_diffs.mean()\n",
        "\n",
        "def training_loop(n_epochs, model, optimizer, params, t_u, t_c):\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "    t_p = model(t_u, *params)\n",
        "    loss = loss_fn(t_p, t_c)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 500 == 0 or epoch == 1:\n",
        "      print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
        "\n",
        "  return params"
      ],
      "metadata": {
        "id": "KZkSKYw0V32C"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = [1e0, 1e-1, 1e-2, 1e-3, 1e-4]\n",
        "final_loss = []\n",
        "\n",
        "for lr in learning_rate:\n",
        "  print(f'Learning rate: {lr}')\n",
        "  model = quadratic_model\n",
        "  params = torch.tensor(np.ones((3,1)), requires_grad=True)\n",
        "  optimizer = optim.Adam([params], lr=lr)\n",
        "  params = training_loop(5000, model, optimizer, params, t_u, t_c)\n",
        "\n",
        "  t_p = quadratic_model(t_u, *params)\n",
        "  loss = loss_fn(t_p, t_c)\n",
        "  final_loss.append(loss)\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1x41imk8ZkFJ",
        "outputId": "528e3e52-6844-4cfa-a4c3-bc5165fcbb06"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning rate: 1.0\n",
            "Epoch 1, Loss 45594787.736440\n",
            "Epoch 500, Loss 63.980121\n",
            "Epoch 1000, Loss 32.234560\n",
            "Epoch 1500, Loss 12.589210\n",
            "Epoch 2000, Loss 4.863224\n",
            "Epoch 2500, Loss 2.968491\n",
            "Epoch 3000, Loss 2.701118\n",
            "Epoch 3500, Loss 2.681979\n",
            "Epoch 4000, Loss 2.681397\n",
            "Epoch 4500, Loss 2.681391\n",
            "Epoch 5000, Loss 2.681391\n",
            "\n",
            "Learning rate: 0.1\n",
            "Epoch 1, Loss 45594787.736440\n",
            "Epoch 500, Loss 5.058848\n",
            "Epoch 1000, Loss 4.218810\n",
            "Epoch 1500, Loss 4.040524\n",
            "Epoch 2000, Loss 3.827470\n",
            "Epoch 2500, Loss 3.593775\n",
            "Epoch 3000, Loss 3.357393\n",
            "Epoch 3500, Loss 3.138764\n",
            "Epoch 4000, Loss 2.957022\n",
            "Epoch 4500, Loss 2.824625\n",
            "Epoch 5000, Loss 2.742930\n",
            "\n",
            "Learning rate: 0.01\n",
            "Epoch 1, Loss 45594787.736440\n",
            "Epoch 500, Loss 21528.203456\n",
            "Epoch 1000, Loss 2136.730671\n",
            "Epoch 1500, Loss 417.591224\n",
            "Epoch 2000, Loss 101.615399\n",
            "Epoch 2500, Loss 27.632278\n",
            "Epoch 3000, Loss 9.728647\n",
            "Epoch 3500, Loss 5.874368\n",
            "Epoch 4000, Loss 5.215224\n",
            "Epoch 4500, Loss 5.101516\n",
            "Epoch 5000, Loss 5.035704\n",
            "\n",
            "Learning rate: 0.001\n",
            "Epoch 1, Loss 45594787.736440\n",
            "Epoch 500, Loss 7626286.411264\n",
            "Epoch 1000, Loss 1968375.136428\n",
            "Epoch 1500, Loss 670115.505700\n",
            "Epoch 2000, Loss 270627.358126\n",
            "Epoch 2500, Loss 121919.263392\n",
            "Epoch 3000, Loss 59036.660741\n",
            "Epoch 3500, Loss 30000.140976\n",
            "Epoch 4000, Loss 15736.469518\n",
            "Epoch 4500, Loss 8417.216935\n",
            "Epoch 5000, Loss 4546.381556\n",
            "\n",
            "Learning rate: 0.0001\n",
            "Epoch 1, Loss 45594787.736440\n",
            "Epoch 500, Loss 37391037.228655\n",
            "Epoch 1000, Loss 30744390.399580\n",
            "Epoch 1500, Loss 25312743.433955\n",
            "Epoch 2000, Loss 20836848.379071\n",
            "Epoch 2500, Loss 17126633.362027\n",
            "Epoch 3000, Loss 14039708.658471\n",
            "Epoch 3500, Loss 11467009.662911\n",
            "Epoch 4000, Loss 9323081.054476\n",
            "Epoch 4500, Loss 7539450.241467\n",
            "Epoch 5000, Loss 6060089.910934\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Final loss for the linear model using Adam optimizer was 2.927648 in the lecture 12 slides\")\n",
        "print(\"\\nNonlinear model final losses:\")\n",
        "for i in range(0, len(final_loss)):\n",
        "  print(f'{learning_rate[i]:.2e} learning rate: {final_loss[i]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uO1M889ZhGs0",
        "outputId": "3440064d-bb17-4e48-fbb5-4869dfd33d10"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final loss for the linear model using Adam optimizer was 2.927648 in the lecture 12 slides\n",
            "\n",
            "Nonlinear model final losses:\n",
            "1.00e+00 learning rate: 2.681390586791368\n",
            "1.00e-01 learning rate: 2.742812543552537\n",
            "1.00e-02 learning rate: 5.035563802413372\n",
            "1.00e-03 learning rate: 4540.802374816841\n",
            "1.00e-04 learning rate: 6057403.528961799\n"
          ]
        }
      ]
    }
  ]
}