{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 419,
      "metadata": {
        "id": "UM_wKxfBkSsu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_model(t_u, *params):\n",
        "    result = 0\n",
        "    for i, p in enumerate(params[:-1]):\n",
        "      result += t_u[i] * p\n",
        "\n",
        "    return result + params[-1:][0]\n",
        "\n",
        "def loss_fn(t_p, t_c):\n",
        "  squared_diffs = (t_p - t_c)**2\n",
        "  return squared_diffs.mean()\n",
        "\n",
        "def training_loop(n_epochs, model, optimizer, params, train_t_u, val_t_u, train_t_c, val_t_c):\n",
        "  final_train_loss = -1\n",
        "  final_val_loss = -1\n",
        "\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "    train_t_p = model(train_t_u, *params)\n",
        "    train_loss = loss_fn(train_t_p, train_t_c)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      val_t_p = model(val_t_u, *params)\n",
        "      val_loss = loss_fn(val_t_p, val_t_c)\n",
        "      assert val_loss.requires_grad == False\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 500 == 0 or epoch <= 3:\n",
        "      print(f'Epoch={epoch}, training loss={train_loss.item():.4f}, validation loss={val_loss.item():.4f}')\n",
        "\n",
        "    if epoch == n_epochs:\n",
        "      final_train_loss = train_loss\n",
        "      final_val_loss = val_loss\n",
        "\n",
        "  return params, final_train_loss, final_val_loss"
      ],
      "metadata": {
        "id": "WxRWsMRuqbMr"
      },
      "execution_count": 420,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://raw.githubusercontent.com/HamedTabkhi/Intro-to-ML/refs/heads/main/Dataset/Housing.csv'\n",
        "dataset = pd.DataFrame(pd.read_csv(url))\n",
        "\n",
        "x = dataset[['area','bedrooms','bathrooms','stories','parking']]\n",
        "y = dataset['price']\n",
        "\n",
        "# scaler = StandardScaler()\n",
        "# x = pd.DataFrame(scaler.fit_transform(x))\n",
        "\n",
        "np.random.seed(0)\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 0.8, test_size = 0.2, random_state = 100)\n",
        "\n",
        "train_t_u = [torch.tensor(x_train[col].values) for col in x_train.columns]\n",
        "val_t_u = [torch.tensor(x_test[col].values) for col in x_test.columns]\n",
        "train_t_c = torch.tensor(y_train.tolist())\n",
        "val_t_c = torch.tensor(y_test.tolist())"
      ],
      "metadata": {
        "id": "jAj26dGbE7yC"
      },
      "execution_count": 421,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = [1e0, 1e-1, 1e-2, 1e-3, 1e-4]\n",
        "final_train_loss = []\n",
        "final_val_loss = []\n",
        "\n",
        "for lr in learning_rate:\n",
        "  print(f'Learning rate: {lr}')\n",
        "  model = linear_model\n",
        "  params = torch.tensor(np.ones((len(train_t_u)+1,1)),dtype=torch.float32, requires_grad=True)\n",
        "  optimizer = optim.Adam([params], lr=lr)\n",
        "  params, train_loss, val_loss = training_loop(5000, model, optimizer, params, train_t_u, val_t_u, train_t_c, val_t_c)\n",
        "  final_train_loss.append(train_loss)\n",
        "  final_val_loss.append(val_loss)\n",
        "  print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLgr1TjjqcFi",
        "outputId": "ea1eea05-2345-4a64-e5ae-53987515e4c7"
      },
      "execution_count": 422,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning rate: 1.0\n",
            "Epoch=1, training loss=26423957913600.0000, validation loss=25105872715776.0000\n",
            "Epoch=2, training loss=26370375680000.0000, validation loss=25053043359744.0000\n",
            "Epoch=3, training loss=26316858458112.0000, validation loss=25000283209728.0000\n",
            "Epoch=500, training loss=9190303072256.0000, validation loss=8320677576704.0000\n",
            "Epoch=1000, training loss=4213831106560.0000, validation loss=3753279750144.0000\n",
            "Epoch=1500, training loss=3426807971840.0000, validation loss=3186221383680.0000\n",
            "Epoch=2000, training loss=3372349390848.0000, validation loss=3199549046784.0000\n",
            "Epoch=2500, training loss=3369634627584.0000, validation loss=3208144224256.0000\n",
            "Epoch=3000, training loss=3367770783744.0000, validation loss=3207288586240.0000\n",
            "Epoch=3500, training loss=3365528666112.0000, validation loss=3205191172096.0000\n",
            "Epoch=4000, training loss=3362905915392.0000, validation loss=3202698182656.0000\n",
            "Epoch=4500, training loss=3359935299584.0000, validation loss=3199867551744.0000\n",
            "Epoch=5000, training loss=3356672131072.0000, validation loss=3196754067456.0000\n",
            "\n",
            "Learning rate: 0.1\n",
            "Epoch=1, training loss=26423957913600.0000, validation loss=25105872715776.0000\n",
            "Epoch=2, training loss=26418595495936.0000, validation loss=25100585795584.0000\n",
            "Epoch=3, training loss=26413235175424.0000, validation loss=25095300972544.0000\n",
            "Epoch=500, training loss=23858669158400.0000, validation loss=22579339329536.0000\n",
            "Epoch=1000, training loss=21500027469824.0000, validation loss=20261772460032.0000\n",
            "Epoch=1500, training loss=19334686572544.0000, validation loss=18139637088256.0000\n",
            "Epoch=2000, training loss=17347155853312.0000, validation loss=16197388075008.0000\n",
            "Epoch=2500, training loss=15524870225920.0000, validation loss=14422387982336.0000\n",
            "Epoch=3000, training loss=13857710931968.0000, validation loss=12804449894400.0000\n",
            "Epoch=3500, training loss=12337541021696.0000, validation loss=11335360315392.0000\n",
            "Epoch=4000, training loss=10957747126272.0000, validation loss=10008403116032.0000\n",
            "Epoch=4500, training loss=9712805347328.0000, validation loss=8817927520256.0000\n",
            "Epoch=5000, training loss=8597913206784.0000, validation loss=7758973763584.0000\n",
            "\n",
            "Learning rate: 0.01\n",
            "Epoch=1, training loss=26423957913600.0000, validation loss=25105872715776.0000\n",
            "Epoch=2, training loss=26423418945536.0000, validation loss=25105344233472.0000\n",
            "Epoch=3, training loss=26422884171776.0000, validation loss=25104817848320.0000\n",
            "Epoch=500, training loss=26157542014976.0000, validation loss=24843231690752.0000\n",
            "Epoch=1000, training loss=25892799643648.0000, validation loss=24582285164544.0000\n",
            "Epoch=1500, training loss=25630122967040.0000, validation loss=24323439984640.0000\n",
            "Epoch=2000, training loss=25369409224704.0000, validation loss=24066580807680.0000\n",
            "Epoch=2500, training loss=25110553559040.0000, validation loss=23811609067520.0000\n",
            "Epoch=3000, training loss=24853478375424.0000, validation loss=23558447169536.0000\n",
            "Epoch=3500, training loss=24598116564992.0000, validation loss=23307034296320.0000\n",
            "Epoch=4000, training loss=24344419893248.0000, validation loss=23057315921920.0000\n",
            "Epoch=4500, training loss=24092354805760.0000, validation loss=22809256394752.0000\n",
            "Epoch=5000, training loss=23841898233856.0000, validation loss=22562838937600.0000\n",
            "\n",
            "Learning rate: 0.001\n",
            "Epoch=1, training loss=26423957913600.0000, validation loss=25105872715776.0000\n",
            "Epoch=2, training loss=26423901290496.0000, validation loss=25105820286976.0000\n",
            "Epoch=3, training loss=26423848861696.0000, validation loss=25105765761024.0000\n",
            "Epoch=500, training loss=26397212934144.0000, validation loss=25079505223680.0000\n",
            "Epoch=1000, training loss=26370438594560.0000, validation loss=25053110468608.0000\n",
            "Epoch=1500, training loss=26343689420800.0000, validation loss=25026734587904.0000\n",
            "Epoch=2000, training loss=26316959121408.0000, validation loss=25000381775872.0000\n",
            "Epoch=2500, training loss=26290243502080.0000, validation loss=24974045741056.0000\n",
            "Epoch=3000, training loss=26263546757120.0000, validation loss=24947730677760.0000\n",
            "Epoch=3500, training loss=26236873080832.0000, validation loss=24921426100224.0000\n",
            "Epoch=4000, training loss=26210209890304.0000, validation loss=24895146688512.0000\n",
            "Epoch=4500, training loss=26183563476992.0000, validation loss=24868877762560.0000\n",
            "Epoch=5000, training loss=26156931743744.0000, validation loss=24842627710976.0000\n",
            "\n",
            "Learning rate: 0.0001\n",
            "Epoch=1, training loss=26423957913600.0000, validation loss=25105872715776.0000\n",
            "Epoch=2, training loss=26423949524992.0000, validation loss=25105866424320.0000\n",
            "Epoch=3, training loss=26423943233536.0000, validation loss=25105862230016.0000\n",
            "Epoch=500, training loss=26421279850496.0000, validation loss=25103236595712.0000\n",
            "Epoch=1000, training loss=26418599690240.0000, validation loss=25100594184192.0000\n",
            "Epoch=1500, training loss=26415919529984.0000, validation loss=25097951772672.0000\n",
            "Epoch=2000, training loss=26413239369728.0000, validation loss=25095307264000.0000\n",
            "Epoch=2500, training loss=26410557112320.0000, validation loss=25092666949632.0000\n",
            "Epoch=3000, training loss=26407879049216.0000, validation loss=25090022440960.0000\n",
            "Epoch=3500, training loss=26405198888960.0000, validation loss=25087375835136.0000\n",
            "Epoch=4000, training loss=26402518728704.0000, validation loss=25084737617920.0000\n",
            "Epoch=4500, training loss=26399838568448.0000, validation loss=25082097303552.0000\n",
            "Epoch=5000, training loss=26397160505344.0000, validation loss=25079452794880.0000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Final losses:\")\n",
        "for i in range(0, len(learning_rate)):\n",
        "  print(f'Learning rate={learning_rate[i]:.2e}, training loss={final_train_loss[i]:.2e}, validation loss={final_val_loss[i]:.2e}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPx1P-rMxNDi",
        "outputId": "6c536ca4-3f55-4684-d737-c9afd347b9a0"
      },
      "execution_count": 423,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final losses:\n",
            "Learning rate=1.00e+00, training loss=3.36e+12, validation loss=3.20e+12\n",
            "Learning rate=1.00e-01, training loss=8.60e+12, validation loss=7.76e+12\n",
            "Learning rate=1.00e-02, training loss=2.38e+13, validation loss=2.26e+13\n",
            "Learning rate=1.00e-03, training loss=2.62e+13, validation loss=2.48e+13\n",
            "Learning rate=1.00e-04, training loss=2.64e+13, validation loss=2.51e+13\n"
          ]
        }
      ]
    }
  ]
}