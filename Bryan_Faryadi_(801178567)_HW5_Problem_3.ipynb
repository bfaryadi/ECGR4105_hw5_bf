{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "UM_wKxfBkSsu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_model(t_u, *params):\n",
        "    result = 0\n",
        "    for i, p in enumerate(params[:-1]):\n",
        "      result += t_u[i] * p\n",
        "\n",
        "    return result + params[-1:][0]\n",
        "\n",
        "def loss_fn(t_p, t_c):\n",
        "  squared_diffs = (t_p - t_c)**2\n",
        "  return squared_diffs.mean()\n",
        "\n",
        "def training_loop(n_epochs, model, optimizer, params, train_t_u, val_t_u, train_t_c, val_t_c):\n",
        "  final_train_loss = -1\n",
        "  final_val_loss = -1\n",
        "\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "    train_t_p = model(train_t_u, *params)\n",
        "    train_loss = loss_fn(train_t_p, train_t_c)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      val_t_p = model(val_t_u, *params)\n",
        "      val_loss = loss_fn(val_t_p, val_t_c)\n",
        "      assert val_loss.requires_grad == False\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 500 == 0 or epoch <= 3:\n",
        "      print(f'Epoch={epoch}, training loss={train_loss.item():.4f}, validation loss={val_loss.item():.4f}')\n",
        "\n",
        "    if epoch == n_epochs:\n",
        "      final_train_loss = train_loss\n",
        "      final_val_loss = val_loss\n",
        "\n",
        "  return params, final_train_loss, final_val_loss"
      ],
      "metadata": {
        "id": "WxRWsMRuqbMr"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://raw.githubusercontent.com/HamedTabkhi/Intro-to-ML/refs/heads/main/Dataset/Housing.csv'\n",
        "dataset = pd.DataFrame(pd.read_csv(url))\n",
        "\n",
        "x = dataset.drop(columns=['price','furnishingstatus'])\n",
        "varlist = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea']\n",
        "\n",
        "def binary_map(x):\n",
        "    return x.map({'yes': 1, 'no': 0})\n",
        "\n",
        "x[varlist] = x[varlist].apply(binary_map)\n",
        "\n",
        "# scaler = StandardScaler()\n",
        "# x = pd.DataFrame(scaler.fit_transform(x))\n",
        "\n",
        "y = dataset['price']\n",
        "\n",
        "np.random.seed(0)\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 0.8, test_size = 0.2, random_state = 100)\n",
        "\n",
        "train_t_u = [torch.tensor(x_train[col].values) for col in x_train.columns]\n",
        "val_t_u = [torch.tensor(x_test[col].values) for col in x_test.columns]\n",
        "train_t_c = torch.tensor(y_train.tolist())\n",
        "val_t_c = torch.tensor(y_test.tolist())"
      ],
      "metadata": {
        "id": "jAj26dGbE7yC"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = [1e0, 1e-1, 1e-2, 1e-3, 1e-4]\n",
        "final_train_loss = []\n",
        "final_val_loss = []\n",
        "\n",
        "for lr in learning_rate:\n",
        "  print(f'Learning rate: {lr}')\n",
        "  model = linear_model\n",
        "  params = torch.tensor(np.ones((len(train_t_u)+1,1)),dtype=torch.float32, requires_grad=True)\n",
        "  optimizer = optim.Adam([params], lr=lr)\n",
        "  params, train_loss, val_loss = training_loop(5000, model, optimizer, params, train_t_u, val_t_u, train_t_c, val_t_c)\n",
        "  final_train_loss.append(train_loss)\n",
        "  final_val_loss.append(val_loss)\n",
        "  print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLgr1TjjqcFi",
        "outputId": "a0b6b435-aad2-4e69-d233-eeb794c0f5cb"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning rate: 1.0\n",
            "Epoch=1, training loss=26423936942080.0000, validation loss=25105851744256.0000\n",
            "Epoch=2, training loss=26370333736960.0000, validation loss=25053001416704.0000\n",
            "Epoch=3, training loss=26316791349248.0000, validation loss=25000218198016.0000\n",
            "Epoch=500, training loss=9185849769984.0000, validation loss=8316158738432.0000\n",
            "Epoch=1000, training loss=4210597298176.0000, validation loss=3749862965248.0000\n",
            "Epoch=1500, training loss=3424268582912.0000, validation loss=3183293497344.0000\n",
            "Epoch=2000, training loss=3369570664448.0000, validation loss=3196197273600.0000\n",
            "Epoch=2500, training loss=3366372507648.0000, validation loss=3204169072640.0000\n",
            "Epoch=3000, training loss=3363909140480.0000, validation loss=3202571042816.0000\n",
            "Epoch=3500, training loss=3360956088320.0000, validation loss=3199595446272.0000\n",
            "Epoch=4000, training loss=3357518594048.0000, validation loss=3196088221696.0000\n",
            "Epoch=4500, training loss=3353641484288.0000, validation loss=3192128536576.0000\n",
            "Epoch=5000, training loss=3349395800064.0000, validation loss=3187784548352.0000\n",
            "\n",
            "Learning rate: 0.1\n",
            "Epoch=1, training loss=26423936942080.0000, validation loss=25105851744256.0000\n",
            "Epoch=2, training loss=26418572427264.0000, validation loss=25100564824064.0000\n",
            "Epoch=3, training loss=26413205815296.0000, validation loss=25095277903872.0000\n",
            "Epoch=500, training loss=23857658331136.0000, validation loss=22578320113664.0000\n",
            "Epoch=1000, training loss=21498179878912.0000, validation loss=20259903897600.0000\n",
            "Epoch=1500, training loss=19332132241408.0000, validation loss=18137057591296.0000\n",
            "Epoch=2000, training loss=17344013271040.0000, validation loss=16194211938304.0000\n",
            "Epoch=2500, training loss=15521248444416.0000, validation loss=14418726354944.0000\n",
            "Epoch=3000, training loss=13853711663104.0000, validation loss=12800408682496.0000\n",
            "Epoch=3500, training loss=12333257588736.0000, validation loss=11331028647936.0000\n",
            "Epoch=4000, training loss=10953256075264.0000, validation loss=10003861733376.0000\n",
            "Epoch=4500, training loss=9708192661504.0000, validation loss=8813263454208.0000\n",
            "Epoch=5000, training loss=8593254907904.0000, validation loss=7754259890176.0000\n",
            "\n",
            "Learning rate: 0.01\n",
            "Epoch=1, training loss=26423936942080.0000, validation loss=25105851744256.0000\n",
            "Epoch=2, training loss=26423402168320.0000, validation loss=25105321164800.0000\n",
            "Epoch=3, training loss=26422863200256.0000, validation loss=25104792682496.0000\n",
            "Epoch=500, training loss=26157420380160.0000, validation loss=24843103764480.0000\n",
            "Epoch=1000, training loss=25892566859776.0000, validation loss=24582052380672.0000\n",
            "Epoch=1500, training loss=25629791617024.0000, validation loss=24323100246016.0000\n",
            "Epoch=2000, training loss=25368973017088.0000, validation loss=24066140405760.0000\n",
            "Epoch=2500, training loss=25110018785280.0000, validation loss=23811063808000.0000\n",
            "Epoch=3000, training loss=24852842938368.0000, validation loss=23557807538176.0000\n",
            "Epoch=3500, training loss=24597386756096.0000, validation loss=23306296098816.0000\n",
            "Epoch=4000, training loss=24343591518208.0000, validation loss=23056474963968.0000\n",
            "Epoch=4500, training loss=24091432058880.0000, validation loss=22808325259264.0000\n",
            "Epoch=5000, training loss=23840879017984.0000, validation loss=22561815527424.0000\n",
            "\n",
            "Learning rate: 0.001\n",
            "Epoch=1, training loss=26423936942080.0000, validation loss=25105851744256.0000\n",
            "Epoch=2, training loss=26423882416128.0000, validation loss=25105797218304.0000\n",
            "Epoch=3, training loss=26423825793024.0000, validation loss=25105746886656.0000\n",
            "Epoch=500, training loss=26397181476864.0000, validation loss=25079473766400.0000\n",
            "Epoch=1000, training loss=26370398748672.0000, validation loss=25053068525568.0000\n",
            "Epoch=1500, training loss=26343636992000.0000, validation loss=25026684256256.0000\n",
            "Epoch=2000, training loss=26316894109696.0000, validation loss=25000316764160.0000\n",
            "Epoch=2500, training loss=26290170101760.0000, validation loss=24973968146432.0000\n",
            "Epoch=3000, training loss=26263464968192.0000, validation loss=24947642597376.0000\n",
            "Epoch=3500, training loss=26236778708992.0000, validation loss=24921335922688.0000\n",
            "Epoch=4000, training loss=26210102935552.0000, validation loss=24895039733760.0000\n",
            "Epoch=4500, training loss=26183446036480.0000, validation loss=24868762419200.0000\n",
            "Epoch=5000, training loss=26156803817472.0000, validation loss=24842499784704.0000\n",
            "\n",
            "Learning rate: 0.0001\n",
            "Epoch=1, training loss=26423936942080.0000, validation loss=25105851744256.0000\n",
            "Epoch=2, training loss=26423928553472.0000, validation loss=25105847549952.0000\n",
            "Epoch=3, training loss=26423922262016.0000, validation loss=25105841258496.0000\n",
            "Epoch=500, training loss=26421254684672.0000, validation loss=25103211429888.0000\n",
            "Epoch=1000, training loss=26418578718720.0000, validation loss=25100566921216.0000\n",
            "Epoch=1500, training loss=26415894364160.0000, validation loss=25097922412544.0000\n",
            "Epoch=2000, training loss=26413214203904.0000, validation loss=25095280001024.0000\n",
            "Epoch=2500, training loss=26410529849344.0000, validation loss=25092637589504.0000\n",
            "Epoch=3000, training loss=26407849689088.0000, validation loss=25089993080832.0000\n",
            "Epoch=3500, training loss=26405167431680.0000, validation loss=25087350669312.0000\n",
            "Epoch=4000, training loss=26402489368576.0000, validation loss=25084708257792.0000\n",
            "Epoch=4500, training loss=26399809208320.0000, validation loss=25082061651968.0000\n",
            "Epoch=5000, training loss=26397124853760.0000, validation loss=25079419240448.0000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Final losses:\")\n",
        "for i in range(0, len(learning_rate)):\n",
        "  print(f'Learning rate={learning_rate[i]:.2e}, training loss={final_train_loss[i]:.2e}, validation loss={final_val_loss[i]:.2e}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPx1P-rMxNDi",
        "outputId": "dfaebc6e-5a9b-4da7-d4ee-3afaa952a7d1"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final losses:\n",
            "Learning rate=1.00e+00, training loss=3.35e+12, validation loss=3.19e+12\n",
            "Learning rate=1.00e-01, training loss=8.59e+12, validation loss=7.75e+12\n",
            "Learning rate=1.00e-02, training loss=2.38e+13, validation loss=2.26e+13\n",
            "Learning rate=1.00e-03, training loss=2.62e+13, validation loss=2.48e+13\n",
            "Learning rate=1.00e-04, training loss=2.64e+13, validation loss=2.51e+13\n"
          ]
        }
      ]
    }
  ]
}